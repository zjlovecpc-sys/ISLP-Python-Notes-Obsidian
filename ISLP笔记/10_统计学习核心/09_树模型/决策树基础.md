---
tags:
  - 统计学习
  - 树模型
  - 回归树
  - 分类树
aliases:
  - Decision Trees
  - CART
  - 递归二叉分裂
  - 基尼系数
  - 信息熵
---

# 决策树基础 (The Basics of Decision Trees)

## 1. 核心思想：分而治之 (Stratification)
决策树的本质是对特征空间 (Predictor Space) 进行**分层**或**分割**。
* 我们将特征空间划分为 $J$ 个互不重叠的矩形区域 $R_1, R_2, \dots, R_J$。
* 对于落入区域 $R_j$ 的任何观测值，我们都给予相同的预测值（通常是该区域内训练样本的平均值或众数）。

## 2. 回归树 (Regression Trees)

### 建树算法：递归二叉分裂 (Recursive Binary Splitting)
为了找到最佳的矩形划分，我们采用一种**自顶向下 (Top-down)** 的**贪心 (Greedy)** 策略：
1.  **自顶向下**：从包含所有数据的根节点开始分裂。
2.  **贪心**：在每一步，只寻找**当前**能让 RSS 下降最多的分裂点，而不考虑这一步对未来的影响。

### 数学目标
寻找切分变量 $X_j$ 和切分点 $s$，定义两个半平面：
$$R_1(j,s) = \{X|X_j < s\} \quad \text{和} \quad R_2(j,s) = \{X|X_j \ge s\}$$
目标是最小化总 RSS：
$$\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2$$
其中 $\hat{y}_{R_k}$ 是区域 $R_k$ 内响应变量的平均值。

## 3. 分类树 (Classification Trees)

对于分类问题，我们不能用 RSS。我们需要衡量节点的**纯度 (Purity)**。

### 三大指标对比
假设 $\hat{p}_{mk}$ 代表第 $m$ 个区域中属于第 $k$ 类的样本比例。

1.  **分类错误率 (Classification Error Rate)**：
    $$E = 1 - \max_k (\hat{p}_{mk})$$
    * *缺点*：对节点纯度的变化不够敏感，通常**不用于**树的生长（分裂），只用于最终的剪枝。

2.  **基尼系数 (Gini Index)**：
    $$G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})$$
    * 衡量总方差。若 $\hat{p}_{mk}$ 接近 0 或 1，则 $G$ 接近 0（纯度高）。
    * **CART 算法默认使用**。

3.  **信息熵 (Entropy / Cross-Entropy)**：
    $$D = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}$$
    * 若节点纯度极高，熵 $\approx 0$。
    * 在数学性质上与 Gini 类似，常用于 C4.5 算法。

## 4. 树 vs 线性模型
* **线性模型**：假设决策边界是线性的（$Y = \beta_0 + \sum \beta_j X_j$）。
* **决策树**：假设决策边界是平行于坐标轴的矩形块。
* **选择依据**：如果真实边界是高度非线性的或阶梯状的，树模型胜出；如果真实边界是线性的，树模型需要很多阶梯去逼近，效果较差。

## 关联笔记
* ➡️ 优化算法：为了防止过拟合，必须进行 [[树的剪枝]]。
* ➡️ 进阶应用：多棵树的集成 [[装袋法与随机森林]]。