---
tags:
  - Pythonå®æˆ˜
  - æ ‘æ¨¡å‹
  - éšæœºæ£®æ—
  - Boosting
  - å‰ªæ
aliases:
  - Tree Lab
  - RandomForest Code
  - XGBoost Code
  - ccp_alpha
---

# æ ‘æ¨¡å‹å®æˆ˜ (Tree-Based Methods)

## 0. ç¼–ç¨‹æ‰‹æ ¸å¿ƒç›´è§‰

* **å•æ£µæ ‘æ˜¯â€œè„†â€çš„**ï¼šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œæ”¹åŠ¨ä¸€ç‚¹æ•°æ®æ ‘ç»“æ„å°±ä¼šå¤§å˜ã€‚å¿…é¡»**å‰ªæ**ã€‚
* **æ£®æ—æ˜¯â€œç¨³â€çš„**ï¼š`max_features` æ˜¯è°ƒèŠ‚ç›¸å…³æ€§çš„å…³é”®ã€‚
    * å›å½’é—®é¢˜ï¼šé»˜è®¤ `max_features=p` (Bagging) æˆ– `p/3` (Random Forest)ã€‚
    * åˆ†ç±»é—®é¢˜ï¼šé»˜è®¤ `max_features=sqrt(p)`ã€‚
* **Boostingæ˜¯â€œç²¾â€çš„**ï¼šå®ƒæ˜¯ä¸²è¡Œçš„ï¼Œä¸ä»…è¦è°ƒæ ‘çš„æ•°é‡ï¼Œè¿˜è¦è°ƒå­¦ä¹ ç‡ (`learning_rate`)ã€‚

---

## 1. å†³ç­–æ ‘ (åˆ†ç±»)

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[å†³ç­–æ ‘åŸºç¡€]] | [[æ ‘çš„å‰ªæ]]

### 1.1 æ•°æ®å‡†å¤‡ä¸ç¼–ç 
æ ‘æ¨¡å‹ä¸èƒ½ç›´æ¥åƒå­—ç¬¦ä¸²ï¼Œéœ€è¦å…ˆå°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºæ•°å€¼ï¼ˆDummy ç¼–ç ï¼‰ã€‚

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from ISLP import load_data, confusion_table

# åŠ è½½ Carseats æ•°æ®é›†
Carseats = load_data('Carseats')

# åˆ›å»ºåˆ†ç±»ç›®æ ‡å˜é‡ High (é”€é‡ > 8)
Carseats['High'] = np.where(Carseats['Sales'] > 8, 'Yes', 'No')

# è½¬æ¢åˆ†ç±»ç‰¹å¾ (ShelveLoc, Urban, US) ä¸ºæ•°å€¼
# drop_first=True æ˜¯ä¸ºäº†é¿å…å®Œå…¨å…±çº¿æ€§ï¼Œè™½ç„¶æ ‘æ¨¡å‹å¯¹æ­¤ä¸æ•æ„Ÿï¼Œä½†è¿™æ˜¯å¥½ä¹ æƒ¯
feature_cols = Carseats.columns.drop(['Sales', 'High'])
X = pd.get_dummies(Carseats[feature_cols], drop_first=True)
y = Carseats['High']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

```

### 1.2 æ‹Ÿåˆä¸å¯è§†åŒ–

`sklearn` æä¾›äº†éå¸¸å¼ºå¤§çš„ç»˜å›¾å·¥å…· `plot_tree`ã€‚

```python
# 1. æ‹Ÿåˆå¤§æ ‘
tree_carseats = DecisionTreeClassifier(criterion='entropy', random_state=0)
tree_carseats.fit(X_train, y_train)

# 2. è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡
print("æœªå‰ªæå‡†ç¡®ç‡:", tree_carseats.score(X_test, y_test))

# 3. å¯è§†åŒ– (æ–‡å­—ç‰ˆ)
print(export_text(tree_carseats, feature_names=list(X.columns)))

# 4. å¯è§†åŒ– (å›¾å½¢ç‰ˆ - è®ºæ–‡ç´ æ)
plt.figure(figsize=(12, 12))
plot_tree(tree_carseats, 
          feature_names=list(X.columns), 
          class_names=['No', 'Yes'], 
          filled=True)
plt.show()

```

### 1.3 ä»£ä»·å¤æ‚æ€§å‰ªæ (Cost Complexity Pruning)

è¿™æ˜¯ Python å®ç°å‰ªæçš„æ ‡å‡†æµç¨‹ï¼šå…ˆç®—è·¯å¾„ï¼Œå†é€šè¿‡ CV é€‰ `ccp_alpha`ã€‚

```python
from sklearn.model_selection import GridSearchCV

# 1. è·å–å‰ªæè·¯å¾„ (è¿”å›ä¸€ç³»åˆ—å€™é€‰çš„ alpha å€¼)
path = tree_carseats.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas
# å»æ‰æœ€å¤§çš„ alpha (å¯¹åº”åªæœ‰æ ¹èŠ‚ç‚¹çš„æ ‘)ï¼Œé¿å…æ— æ„ä¹‰æœç´¢
ccp_alphas = ccp_alphas[:-1]

# 2. é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³ alpha
grid = GridSearchCV(
    DecisionTreeClassifier(criterion='entropy', random_state=0),
    param_grid={'ccp_alpha': ccp_alphas},
    cv=5,
    scoring='accuracy'
)
grid.fit(X_train, y_train)

# 3. è¾“å‡ºç»“æœ
print("æœ€ä½³å‰ªæå‚æ•° ccp_alpha:", grid.best_params_['ccp_alpha'])
best_tree = grid.best_estimator_
print("å‰ªæåå‡†ç¡®ç‡:", best_tree.score(X_test, y_test))

# 4. ç”»å‡ºå‰ªæåçš„æ ‘ (é€šå¸¸æ›´ç®€æ´ï¼Œé€‚åˆæ”¾å…¥è®ºæ–‡)
plt.figure(figsize=(12, 8))
plot_tree(best_tree, feature_names=list(X.columns), class_names=['No', 'Yes'], filled=True)
plt.show()

```

---

## 2. å†³ç­–æ ‘ (å›å½’)

å›å½’æ ‘çš„ä»£ç é€»è¾‘å‡ ä¹ä¸€è‡´ï¼Œåªéœ€æ›´æ¢ä¸º `DecisionTreeRegressor`ã€‚

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

Boston = load_data('Boston')
X = Boston.drop('medv', axis=1)
y = Boston['medv']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# æ‹Ÿåˆå›å½’æ ‘
tree_boston = DecisionTreeRegressor(max_depth=3, random_state=0)
tree_boston.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
y_pred = tree_boston.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))

# åŒæ ·å¯ä»¥ä½¿ç”¨ cost_complexity_pruning_path è¿›è¡Œå‰ªæ

```

---

## 3. éšæœºæ£®æ—ä¸ Bagging

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[è£…è¢‹æ³•ä¸éšæœºæ£®æ—]]

åœ¨ `sklearn` ä¸­ï¼ŒBagging æ˜¯ Random Forest çš„ä¸€ç§ç‰¹ä¾‹ (`max_features` å–å…¨éƒ¨ç‰¹å¾)ã€‚

```python
from sklearn.ensemble import RandomForestRegressor

# --- Bagging (m = p) ---
# max_features=X.shape[1] æ„å‘³ç€æ¯æ¬¡åˆ†è£‚éƒ½è€ƒè™‘æ‰€æœ‰ç‰¹å¾
bag_boston = RandomForestRegressor(
    max_features=X_train.shape[1], 
    random_state=0, 
    n_estimators=500  # æ ‘çš„æ•°é‡
)
bag_boston.fit(X_train, y_train)
print("Bagging MSE:", mean_squared_error(y_test, bag_boston.predict(X_test)))

# --- Random Forest (m < p) ---
# é»˜è®¤ max_features='sqrt' (åˆ†ç±») æˆ– 1.0 (å›å½’ï¼Œå³Bagging)
# æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æŒ‡å®š m = p/3 (å›å½’å¸¸ç”¨ç»éªŒå€¼)
m_try = int(X_train.shape[1] / 3)
rf_boston = RandomForestRegressor(
    max_features=m_try, 
    random_state=0, 
    n_estimators=500
)
rf_boston.fit(X_train, y_train)
print("Random Forest MSE:", mean_squared_error(y_test, rf_boston.predict(X_test)))

# --- ç‰¹å¾é‡è¦æ€§ (Feature Importance) ---
# è¿™æ˜¯å†™è®ºæ–‡åˆ†æçš„å…³é”®ï¼
importances = pd.Series(rf_boston.feature_importances_, index=X.columns)
importances.sort_values(ascending=False).plot(kind='barh')
plt.title("Feature Importance (Random Forest)")

```

---

## 4. æå‡æ³• (Boosting)

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[æå‡æ³•Boosting]]

Boosting éœ€è¦è°ƒå‚ï¼å°¤å…¶æ˜¯ `learning_rate` å’Œ `n_estimators`ã€‚

```python
from sklearn.ensemble import GradientBoostingRegressor

# æ‹Ÿåˆ Boosting æ¨¡å‹
# learning_rate: å­¦ä¹ ç‡ (lambda)ï¼Œè¶Šå°è¶Šç¨³ï¼Œä½†éœ€è¦æ›´å¤šæ ‘
# n_estimators: æ ‘çš„æ•°é‡ (B)
# max_depth: äº¤äº’æ·±åº¦ (d)ï¼Œé€šå¸¸ 1-4 ä¹‹é—´
boost_boston = GradientBoostingRegressor(
    n_estimators=5000, 
    learning_rate=0.001, 
    max_depth=3, 
    random_state=0
)

boost_boston.fit(X_train, y_train)
print("Boosting MSE:", mean_squared_error(y_test, boost_boston.predict(X_test)))

# éƒ¨åˆ†ä¾èµ–å›¾ (Partial Dependence Plot)
# ç±»ä¼¼äº GAM ä¸­çš„å›¾ï¼Œå±•ç¤ºå˜é‡å¯¹ç»“æœçš„éçº¿æ€§å½±å“
from sklearn.inspection import PartialDependenceDisplay

fig, ax = plt.subplots(figsize=(8, 8))
# æŸ¥çœ‹ 'lstat' å’Œ 'rm' è¿™ä¸¤ä¸ªé‡è¦å˜é‡çš„å½±å“
PartialDependenceDisplay.from_estimator(boost_boston, X_train, ['lstat', 'rm'], ax=ax)
plt.show()

```

---

## 5. è´å¶æ–¯åŠ æ€§å›å½’æ ‘ (BART)

*æ³¨ï¼šè™½ç„¶è§†é¢‘ä¸­æœªè¯¦ç»†å±•å¼€ï¼Œä½†è¿™æ˜¯ç¬¬ 8 ç« çš„å‰æ²¿å†…å®¹ã€‚`ISLP` åº“æä¾›äº† Python å®ç°ã€‚*

```python
from ISLP.bart import BART

# BART ä¸éœ€è¦å¤ªå¤šè°ƒå‚ï¼Œå¼€ç®±å³ç”¨
bart_boston = BART(random_state=0, burnin=500, ndraw=2000)
bart_boston.fit(X_train, y_train)

# é¢„æµ‹
y_pred_bart = bart_boston.predict(X_test)
print("BART MSE:", mean_squared_error(y_test, y_pred_bart))

```

---

## å¸¸è§é”™è¯¯æ¸…å•

| ç°è±¡ | åŸå› è¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
| --- | --- | --- |
| **æ ‘å›¾ç”»å‡ºæ¥æ˜¯ä¸€å›¢é»‘** | æ ‘å¤ªæ·±ï¼ŒèŠ‚ç‚¹å¤ªå¤šã€‚ | é™åˆ¶ `max_depth` æˆ–è¿›è¡Œå‰ªæåå†ç”»ã€‚ |
| **Boosting è¿‡æ‹Ÿåˆ** | æ ‘çš„æ•°é‡ `n_estimators` å¤ªå¤šï¼Œä¸”å­¦ä¹ ç‡å¤ªå¤§ã€‚ | é™ä½ `learning_rate`ï¼Œæˆ–ç”¨ CV ç¡®å®šæœ€ä½³æ ‘æ•°é‡ã€‚ |
| **åˆ†ç±»å˜é‡æŠ¥é”™** | `sklearn` çš„æ ‘æ¨¡å‹ä¸æ”¯æŒå­—ç¬¦ä¸²è¾“å…¥ã€‚ | å¿…é¡»å…ˆç”¨ `pd.get_dummies` ç¼–ç ã€‚ |
| **RandomForest æ•ˆæœä¸å¦‚å•æ£µæ ‘** | æä¸ºç½•è§ï¼Œé€šå¸¸æ˜¯ `n_estimators` å¤ªå°‘ã€‚ | å¢åŠ æ ‘çš„æ•°é‡ (RF ä¸ä¼šè¿‡æ‹Ÿåˆ)ã€‚ |
