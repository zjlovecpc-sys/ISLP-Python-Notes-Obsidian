---
tags:
  - 统计学习
  - 树模型
  - 集成学习
  - 随机森林
aliases:
  - Bagging
  - Random Forests
  - OOB Error
  - 袋外误差
  - 特征去相关
---

# 装袋法与随机森林 (Bagging and Random Forests)

## 1. 装袋法 (Bagging)

### 核心原理：平均降低方差
决策树的主要缺点是**高方差**。统计学基本原理告诉我们：对 $N$ 个独立同分布的随机变量取平均，其方差降为 $\sigma^2/N$。
* **Bootstrap Aggregation (Bagging)**：既然只有一个训练集，我们通过 [[自助法]] (Bootstrap) 生成 $B$ 个不同的训练集，分别训练 $B$ 棵树，然后取平均（回归）或投票（分类）。
  $$\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x)$$

### 编程手利器：袋外误差 (OOB Error)
* **原理**：在构建每棵树时，大约有 1/3 的观测值未被抽中（Out-of-Bag）。
* **用法**：我们可以直接利用这部分数据来评估模型性能。
* **结论**：**对于 Bagging 和随机森林，不需要专门做交叉验证 (CV)。** OOB 误差就是测试误差的无偏估计。这在比赛中能节省大量计算时间。

---

## 2. 随机森林 (Random Forests)

### 核心改进：特征去相关 (Decorrelating)
Bagging 有一个致命缺陷：如果数据中有一个极强的预测变量（Super Predictor），所有 $B$ 棵树都会优先选它做第一次分裂。导致所有树长得都很像（高度相关）。**高度相关的变量取平均，降低方差的效果大打折扣。**

随机森林在每次分裂时，**强制**只从总共 $p$ 个特征中随机选取 $m$ 个候选特征。
* **回归问题**：推荐 $m \approx p/3$。
* **分类问题**：推荐 $m \approx \sqrt{p}$。

### 为什么这也行？
这迫使树去考虑那些次优的预测变量。虽然单棵树变弱了，但因为树与树之间的**相关性大幅降低**，集成的整体方差反而更低。

## 关联笔记
* ⬅️ 基础技术：[[自助法]] (Bootstrap)
* ⬅️ 基础单元：[[决策树基础]]
* ➡️ 另一条路：[[提升法Boosting]] (通过降低偏差来提升性能)