---
tags:
  - 统计学习
  - 树模型
  - 贝叶斯方法
  - BART
aliases:
  - Bayesian Additive Regression Trees
  - MCMC
  - 贝叶斯树
---

# 贝叶斯加性回归树 (BART)

## 1. 核心定义：概率视角
BART 也是一种**加性模型**，将预测值表示为 $K$ 棵树的和：
$$f(x) = \sum_{k=1}^K g(x; T_k, M_k)$$
与 Boosting 不同的是，BART 不通过梯度下降更新，而是通过**贝叶斯概率模型**来更新。
* **先验分布 (Priors)**：我们给每棵树施加一个极其严苛的先验，强制它们做“弱学习器”（比如树很浅）。
* **似然函数**：假设残差服从正态分布。

## 2. 拟合算法：MCMC (马尔可夫链蒙特卡洛)
BART 使用**后向拟合 (Backfitting)** 的贝叶斯版本。在每一轮迭代中，我们试图修改第 $k$ 棵树 $T_k$，同时保持其他 $K-1$ 棵树不变。
修改动作包括：
1.  **生长 (Grow)**：增加一个分裂。
2.  **修剪 (Prune)**：删掉一个分裂。
3.  **改变 (Change)**：改变切分规则。
4.  **旋转 (Rotate)**：(有些实现包含) 改变切分变量。

## 3. 编程手直觉：Burn-in
因为是 MCMC 算法，初始的迭代结果通常不稳定。
* **Burn-in**：我们需要丢弃前 $L$ 次迭代（如前 1000 次），只取后续迭代的平均值作为预测结果。
* **优势**：除了给出预测值，BART 还能自然地给出**置信区间 (Credible Intervals)**，量化预测的不确定性。这对美赛中通过不确定性分析来提升论文档次非常有帮助。

## 4. 对比总结
| 模型 | 树的关系 | 核心策略 | 优点 |
| :--- | :--- | :--- | :--- |
| **随机森林** | 独立 | 平均去相关 | 稳健，并行化，无须调参 |
| **Boosting** | 依赖 | 拟合残差 | 精度最高，需精细调参 |
| **BART** | 依赖 | 贝叶斯采样 | 自带置信区间，对参数不敏感 |

## 关联笔记
* ⬅️ 基础框架：[[决策树基础]]
* 🔗 算法基础：[[马尔可夫链蒙特卡洛MCMC（待后期完善）]]