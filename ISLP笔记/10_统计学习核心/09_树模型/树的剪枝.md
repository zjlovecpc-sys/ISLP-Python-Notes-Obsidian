---
tags:
  - 统计学习
  - 树模型
  - 正则化
  - 模型选择
aliases:
  - Tree Pruning
  - Cost Complexity Pruning
  - 代价复杂性剪枝
  - Weakest Link Pruning
---

# 树的剪枝 (Tree Pruning)

## 1. 核心痛点：过拟合
如果允许决策树一直生长，直到每个叶节点只有一个样本，由于 RSS 为 0，训练误差极低。但这会导致模型极其复杂，捕获了所有噪声，**方差极大**，测试效果极差。
* **对策**：我们需要一棵更小的树（更少的叶节点）。

## 2. 为什么不能“早停” (Stop Early)?
如果在分裂带来的 RSS 下降小于某个阈值时就停止（预剪枝），可能会因为**短视**而错过后面一次非常好的分裂（即该分裂本身收益小，但它开启了后续的高收益分裂）。
* **结论**：先让树长成茂盛的**大树 ($T_0$)**，然后再从底部**剪枝 (Prune)**。

## 3. 代价复杂性剪枝 (Cost Complexity Pruning)
也称为**最弱环节剪枝 (Weakest Link Pruning)**。

### 目标函数
我们不直接看测试误差（需要交叉验证，计算量太大），而是定义一个类似 Lasso 的正则化目标函数：
$$R_{\alpha}(T) = \sum_{m=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|$$
* **第一项**：训练数据的 RSS（拟合程度）。
* **第二项**：惩罚项。$|T|$ 是树的终端节点（叶子）数量。
* **$\alpha$ (Tuning Parameter)**：衡量“树的复杂度”的代价。
    * $\alpha = 0 \implies T = T_0$ (完整大树)。
    * $\alpha \to \infty \implies T =$ 只有根节点的树。

### 算法流程
1.  **生长**：在训练集上构建一棵完整的大树 $T_0$。
2.  **路径生成**：随着 $\alpha$ 从 0 逐渐增大，我们剪掉那些“性价比”最低的分枝（对 RSS 贡献小，但占用了叶节点配额）。这会产生一个嵌套的子树序列 $T_1, T_2, \dots$。
3.  **交叉验证 (CV)**：
    * 使用 K-折交叉验证。
    * 对于每一折，重复步骤 1-2。
    * 计算不同 $\alpha$ 下的平均测试误差。
4.  **选择**：选出 CV 误差最小的 $\alpha$，并从第 2 步生成的子树序列中选出对应的最优子树。

## 编程手实战
在 `sklearn` 的 `DecisionTreeRegressor` 或 `Classifier` 中，核心参数是 `ccp_alpha`。
* **操作**：先用 `path = clf.cost_complexity_pruning_path(X, y)` 获取候选的 `ccp_alphas`，然后通过 CV 选择最佳值。

## 关联笔记
* 🔗 理论相似：[[套索回归]] (L1 正则化思想的延续)。
* ➡️ 下一步：剪枝只能降低方差，若想进一步提升精度，需使用 [[装袋法与随机森林]]。